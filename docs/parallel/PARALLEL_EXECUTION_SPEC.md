# ğŸ”€ Parallel Execution Feature Specification

**Version:** 1.0  
**Date:** January 23, 2026  
**Status:** Draft

---

## ğŸ“‹ Executive Summary

This specification defines a parallel execution system for independent events in the RH-Agents framework. Currently, while the framework uses `async/await`, it executes events sequentially. This feature will enable controlled parallel execution of independent events with proper concurrency management, progress tracking, and organized event emission.

---

## ğŸ¯ Problem Statement

### Current State
- **Sequential Execution**: Despite being async, events execute one after another
- **No Concurrency Control**: No built-in mechanism to limit parallel workers
- **Messy Output**: EventPrinter becomes disorganized when parallel execution is attempted externally
- **No Progress Tracking**: No visibility into which parallel events have completed
- **Manual Orchestration**: Users must manually use `asyncio.gather()` outside the framework

### Issues Identified

1. **Event Bus**: Currently handles events sequentially via `publish()` method
   - Subscribers called one by one with `await`
   - No queue-based async processing for handlers
   
2. **EventPrinter**: Designed for linear execution
   - Uses address depth for indentation
   - No support for concurrent event visualization
   - No progress indicators for parallel groups
   
3. **Execution State**: No concept of parallel execution groups
   - No tracking of which events can run in parallel
   - No mechanism to gather results from parallel events

---

## ğŸ” Technical Analysis

### Event Bus Capabilities

**Current Implementation:**
```python
class EventBus(BaseModel):
    subscribers: list[Callable] = Field(default_factory=list)
    events: list[Any] = Field(default_factory=list)
    queue: asyncio.Queue = Field(default_factory=asyncio.Queue)

    async def publish(self, event: ExecutionEvent):
        self.events.append(event)
        for handler in self.subscribers:
            event_copy = event.model_copy()
            result = handler(event_copy)
            if asyncio.iscoroutine(result):
                await result
        await asyncio.sleep(0)
```

**Analysis:**
- âœ… Has `asyncio.Queue` but **unused** for async processing
- âœ… Supports async handlers (coroutines)
- âœ… Has `stream()` method for async iteration
- âŒ Sequential handler execution
- âŒ No batching or parallel casting

**Verdict:** Event bus **CAN** handle async casting with modifications to use the queue for parallel event processing.

### EventPrinter Design

**Current Implementation:**
- Uses address depth (`::` count) for indentation
- Prints immediately when event received
- Tracks statistics globally (not per parallel group)
- No concept of event groups or batches

**Challenges:**
1. Parallel events at same depth will interleave output
2. No visual grouping for parallel operations
3. Progress tracking requires new design
4. Statistics need parallel-aware aggregation

---

## ğŸ’¡ Proposed Solution

### Core Components

#### 1. Parallel Execution Manager
Orchestrates parallel event execution with concurrency control.

```python
class ParallelExecutionManager:
    """Manages parallel execution of independent events with worker limits."""
    
    def __init__(
        self,
        execution_state: ExecutionState,
        max_workers: int = 5,
        batch_mode: BatchMode = BatchMode.GATHER
    ):
        self.semaphore = asyncio.Semaphore(max_workers)
        self.execution_state = execution_state
        self.batch_mode = batch_mode
    
    async def execute_parallel(
        self,
        events: list[ExecutionEvent],
        dependencies: dict[int, list[int]] = None
    ) -> list[Any]:
        """Execute events in parallel respecting dependencies."""
        pass
```

#### 2. Parallel Event Group
Represents a group of events that can execute in parallel.

```python
class ParallelEventGroup(BaseModel):
    """Group of events that execute in parallel."""
    
    group_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str = Field(default="Parallel Group")
    events: list[ExecutionEvent] = Field(default_factory=list)
    total: int = Field(default=0)
    completed: int = Field(default=0)
    failed: int = Field(default=0)
    status: GroupStatus = Field(default=GroupStatus.PENDING)
    start_time: Optional[float] = None
    end_time: Optional[float] = None
```

#### 3. Enhanced Event Metadata
Add parallel execution metadata to ExecutionEvent.

```python
class ExecutionEvent:
    # ... existing fields ...
    
    # Parallel execution fields
    group_id: Optional[str] = Field(default=None)
    parallel_index: Optional[int] = Field(default=None)
    is_parallel: bool = Field(default=False)
```

#### 4. Progress-Aware EventPrinter
Enhanced printer with parallel execution support.

```python
class ParallelEventPrinter(EventPrinter):
    """Event printer with parallel execution awareness."""
    
    def __init__(self, 
                 show_progress: bool = True,
                 collapse_parallel: bool = False,
                 **kwargs):
        super().__init__(**kwargs)
        self.show_progress = show_progress
        self.collapse_parallel = collapse_parallel
        self.parallel_groups: dict[str, ParallelGroupState] = {}
```

---

## ğŸ¨ Implementation Options

### DECISION POINT 1: Parallel Execution API Design

**Question:** How should users define and execute parallel events?

#### Option A: Explicit Parallel Wrapper

**Example:**
```python
from rh_agents.parallel import ParallelExecutor

parallel = ParallelExecutor(max_workers=5)
results = await parallel.gather([
    event1(input1, context, state),
    event2(input2, context, state),
    event3(input3, context, state)
])
```

**Pros:**
- âœ… Explicit and clear intent
- âœ… Easy to understand and debug
- âœ… Compatible with existing code
- âœ… Full control over parallelism

**Cons:**
- âŒ Requires code changes
- âŒ More boilerplate
- âŒ Manual dependency management

---

#### Option B: Declarative Dependencies in Actors

**Example:**
```python
class MyAgent(Agent):
    dependencies = []  # Empty = can run in parallel
    parallel_group = "data_processing"
    
# Framework auto-detects and parallelizes
```

**Pros:**
- âœ… Declarative and clean
- âœ… Framework handles orchestration
- âœ… Less user code
- âœ… Automatic optimization

**Cons:**
- âŒ Less explicit control
- âŒ Harder to debug
- âŒ Magic behavior
- âŒ Requires actor redesign

---

#### Option C: Context Manager API

**Example:**
```python
async with execution_state.parallel(max_workers=3) as parallel:
    parallel.add(event1(input1, context, state))
    parallel.add(event2(input2, context, state))
    parallel.add(event3(input3, context, state))
    # Automatically gathers on context exit

# Or one-liner
results = await execution_state.parallel_execute([
    event1(input1, context, state),
    event2(input2, context, state)
], max_workers=3)
```

**Pros:**
- âœ… Clean and Pythonic
- âœ… Context management benefits
- âœ… Automatic cleanup
- âœ… Flexible

**Cons:**
- âŒ May be unfamiliar pattern
- âŒ Context manager overhead

---

**Suggested Approach:** **Option C (Context Manager)** - Provides the best balance of clarity, Pythonic design, and flexibility while keeping the API clean.

**Your Decision:**
```
[ ] Option A - Explicit Parallel Wrapper
[ ] Option B - Declarative Dependencies
[X] Option C - Context Manager API
[ ] Other (describe): _________________
```

---

### DECISION POINT 2: Event Organization & Printing

**Question:** How should parallel events be displayed in the EventPrinter?

#### Option A: Interleaved Real-Time Output

**Example:**
```
  â–¶ ğŸ”§ ProcessDoc1 [STARTED]
  â–¶ ğŸ”§ ProcessDoc2 [STARTED]
  â–¶ ğŸ”§ ProcessDoc3 [STARTED]
  âœ” ğŸ”§ ProcessDoc1 [COMPLETED] (1.2s)
  âœ” ğŸ”§ ProcessDoc3 [COMPLETED] (1.5s)
  âœ” ğŸ”§ ProcessDoc2 [COMPLETED] (2.1s)
```

**Pros:**
- âœ… Real-time feedback
- âœ… Shows actual execution order
- âœ… Simple implementation
- âœ… Familiar pattern

**Cons:**
- âŒ Can be messy/confusing
- âŒ Hard to track groups
- âŒ Lost visual hierarchy

---

#### Option B: Grouped with Progress Bar

**Example:**
```
  â–¶ ğŸ“¦ Parallel Group: Document Processing [0/3]
    â–¶ ğŸ”§ ProcessDoc1 [STARTED]
    â–¶ ğŸ”§ ProcessDoc2 [STARTED]
    â–¶ ğŸ”§ ProcessDoc3 [STARTED]
  
  ğŸ“¦ Parallel Group: Document Processing [1/3] â–ˆâ–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–’ 33%
    âœ” ğŸ”§ ProcessDoc1 [COMPLETED] (1.2s)
  
  ğŸ“¦ Parallel Group: Document Processing [2/3] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’â–’ 67%
    âœ” ğŸ”§ ProcessDoc3 [COMPLETED] (1.5s)
  
  âœ” ğŸ“¦ Parallel Group: Document Processing [3/3] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% (2.1s)
    âœ” ğŸ”§ ProcessDoc2 [COMPLETED] (2.1s)
```

**Pros:**
- âœ… Clear visual grouping
- âœ… Progress visibility
- âœ… Easy to track completion
- âœ… Professional appearance

**Cons:**
- âŒ More complex implementation
- âŒ Requires buffering/state
- âŒ Terminal refresh overhead

---

#### Option C: Collapsed with Summary

**Example:**
```
  â–¶ ğŸ“¦ Parallel Group: Document Processing (3 events)
    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% (2.1s)
    âœ” 3 completed, 0 failed
    â””â”€ View details: execution_state.history.get_group("doc_processing")
```

**Pros:**
- âœ… Clean and minimal
- âœ… Reduces noise
- âœ… Good for many parallel events
- âœ… Fast rendering

**Cons:**
- âŒ Less visibility
- âŒ Details hidden
- âŒ Harder to debug
- âŒ May miss failures

---

#### Option D: Hybrid Mode (Configurable)

**Example:**
```python
printer = ParallelEventPrinter(
    parallel_mode="progress",  # or "realtime", "collapsed"
    show_parallel_details=True
)
```

Different modes:
- `realtime`: Option A behavior
- `progress`: Option B behavior  
- `collapsed`: Option C behavior
- `smart`: Auto-choose based on event count

**Pros:**
- âœ… Flexibility for different use cases
- âœ… User can choose preference
- âœ… Best of all worlds
- âœ… Adaptable

**Cons:**
- âŒ Most complex implementation
- âŒ More configuration options
- âŒ Testing overhead

---

**Suggested Approach:** **Option D (Hybrid Mode)** - Provides maximum flexibility with sensible defaults. Start with "progress" and "realtime" modes, add others later.

**Your Decision:**
```
[ ] Option A - Interleaved Real-Time
[ ] Option B - Grouped with Progress Bar
[ ] Option C - Collapsed with Summary
[X] Option D - Hybrid Mode (Configurable)
[ ] Other (describe): _________________
```

---

### DECISION POINT 3: Concurrency Control Mechanism

**Question:** How should we control the number of parallel workers?

#### Option A: Asyncio Semaphore (Global)

**Example:**
```python
semaphore = asyncio.Semaphore(max_workers)

async def execute_with_limit(event):
    async with semaphore:
        return await event()
```

**Pros:**
- âœ… Simple and standard
- âœ… Built-in asyncio
- âœ… Reliable
- âœ… Well-tested

**Cons:**
- âŒ Global limit (affects all parallel groups)
- âŒ No prioritization
- âŒ No dynamic adjustment

---

#### Option B: Per-Group Semaphores

**Example:**
```python
class ParallelGroup:
    def __init__(self, max_workers: int):
        self.semaphore = asyncio.Semaphore(max_workers)
```

**Pros:**
- âœ… Isolated limits per group
- âœ… Flexible control
- âœ… Better resource management
- âœ… Independent scaling

**Cons:**
- âŒ Can exceed global resources
- âŒ More complex tracking
- âŒ Need coordination

---

#### Option C: Token Bucket / Rate Limiter

**Example:**
```python
class TokenBucket:
    def __init__(self, rate: float, capacity: int):
        self.rate = rate
        self.capacity = capacity
    
    async def acquire(self):
        # Wait for token availability
```

**Pros:**
- âœ… Smooth rate limiting
- âœ… Prevents bursts
- âœ… Better for external APIs
- âœ… Professional approach

**Cons:**
- âŒ More complex
- âŒ Overkill for CPU tasks
- âŒ Additional configuration
- âŒ Harder to reason about

---

#### Option D: Hierarchical Limits

**Example:**
```python
ExecutionState(
    global_max_workers=10,
    per_group_max_workers=3
)
# Max 10 total, max 3 per group
```

**Pros:**
- âœ… Flexible control
- âœ… Prevents resource monopoly
- âœ… Good for multi-tenant
- âœ… Fine-grained control

**Cons:**
- âŒ Most complex
- âŒ Configuration complexity
- âŒ Harder to optimize
- âŒ Potential deadlocks

---

**Suggested Approach:** **Option B (Per-Group Semaphores)** with an optional global limit for safety. Provides good isolation while allowing fine-grained control.

**Your Decision:**
```
[ ] Option A - Asyncio Semaphore (Global)
[X] Option B - Per-Group Semaphores
[ ] Option C - Token Bucket / Rate Limiter
[ ] Option D - Hierarchical Limits
[ ] Other (describe): _________________
```

---

### DECISION POINT 4: Result Gathering Strategy

**Question:** How should results from parallel events be collected and made available?

#### Option A: Return List (Order Preserved)

**Example:**
```python
results = await parallel_execute([event1, event2, event3])
# results[0] = event1 result
# results[1] = event2 result
# results[2] = event3 result
```

**Pros:**
- âœ… Simple and predictable
- âœ… Order preserved
- âœ… Standard asyncio pattern
- âœ… Easy to use

**Cons:**
- âŒ No access to individual results until all complete
- âŒ One failure can block access
- âŒ No streaming results

---

#### Option B: Return Dictionary (Named Results)

**Example:**
```python
results = await parallel_execute({
    "doc1": event1,
    "doc2": event2,
    "doc3": event3
})
# results["doc1"] = event1 result
```

**Pros:**
- âœ… Named access
- âœ… Self-documenting
- âœ… No index confusion
- âœ… Better for large groups

**Cons:**
- âŒ Requires naming
- âŒ More verbose
- âŒ Still blocks on all complete

---

#### Option C: AsyncIterator (Stream Results)

**Example:**
```python
async for result in parallel_execute_stream([event1, event2, event3]):
    # Process result as soon as available
    print(f"Got result: {result}")
```

**Pros:**
- âœ… Results available immediately
- âœ… Memory efficient
- âœ… Can process while others run
- âœ… Great for pipelines

**Cons:**
- âŒ Order not guaranteed
- âŒ More complex usage
- âŒ Harder error handling
- âŒ Different paradigm

---

#### Option D: Multiple Strategies (Configurable)

**Example:**
```python
# List mode (default)
results = await parallel.gather([...])

# Dict mode
results = await parallel.gather_dict({"name": event})

# Stream mode
async for result in parallel.gather_stream([...]):
    pass

# As completed (with metadata)
async for result in parallel.as_completed([...]):
    print(f"Event {result.event_id} completed")
```

**Pros:**
- âœ… Maximum flexibility
- âœ… Choose best for use case
- âœ… Compatible patterns
- âœ… Future-proof

**Cons:**
- âŒ More API surface
- âŒ More to document
- âŒ Testing overhead
- âŒ Choice paralysis

---

**Suggested Approach:** **Option D (Multiple Strategies)** - Start with list and stream modes, add dictionary mode if needed. Most users need list, but streaming is powerful for long-running operations.

**Your Decision:**
```
[ ] Option A - Return List (Order Preserved)
[ ] Option B - Return Dictionary (Named Results)
[ ] Option C - AsyncIterator (Stream Results)
[X] Option D - Multiple Strategies (Configurable)
[ ] Other (describe): _________________
```

---

### DECISION POINT 5: Error Handling Strategy

**Question:** How should errors in parallel events be handled?

#### Option A: Fail Fast (Cancel All)

**Example:**
```python
# If one fails, all are cancelled
try:
    results = await parallel_execute([event1, event2, event3])
except Exception as e:
    # All other events cancelled
```

**Pros:**
- âœ… Quick failure detection
- âœ… No wasted work
- âœ… Simple error handling
- âœ… Safe default

**Cons:**
- âŒ Loses successful results
- âŒ Aggressive cancellation
- âŒ No partial success

---

#### Option B: Fail Slow (Collect All, Report Errors)

**Example:**
```python
results = await parallel_execute([event1, event2, event3])
# results = [
#   ExecutionResult(ok=True, result=...),
#   ExecutionResult(ok=False, error=...),
#   ExecutionResult(ok=True, result=...)
# ]
```

**Pros:**
- âœ… Get all results
- âœ… Partial success possible
- âœ… Can analyze failures
- âœ… More resilient

**Cons:**
- âŒ Continues on failure
- âŒ May waste resources
- âŒ Harder error handling

---

#### Option C: Configurable Strategy

**Example:**
```python
await parallel_execute(
    [event1, event2, event3],
    error_strategy=ErrorStrategy.FAIL_FAST  # or COLLECT_ALL, FAIL_THRESHOLD
)
```

**Pros:**
- âœ… Flexible control
- âœ… Best for different scenarios
- âœ… User decides
- âœ… Advanced options (e.g., threshold)

**Cons:**
- âŒ More configuration
- âŒ More complex
- âŒ Need to choose

---

#### Option D: Exception Groups (Python 3.11+)

**Example:**
```python
try:
    results = await parallel_execute([event1, event2, event3])
except ExceptionGroup as eg:
    for error in eg.exceptions:
        print(f"Error: {error}")
    # Still have partial results
```

**Pros:**
- âœ… Modern Python feature
- âœ… Clean error handling
- âœ… Multiple errors preserved
- âœ… Pythonic

**Cons:**
- âŒ Python 3.11+ only
- âŒ New concept for many
- âŒ May need polyfill

---

**Suggested Approach:** **Option C (Configurable)** with **Option B (Fail Slow)** as default. Most parallel operations benefit from partial success, but users should be able to opt into fail-fast for critical operations.

**Your Decision:**
```
[ ] Option A - Fail Fast (Cancel All)
[ ] Option B - Fail Slow (Collect All)
[X] Option C - Configurable Strategy
[ ] Option D - Exception Groups
[ ] Other (describe): _________________
```

---

## ğŸ—ï¸ Proposed Architecture

### Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Code                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  async with execution_state.parallel() as p:        â”‚   â”‚
â”‚  â”‚      p.add(event1(...))                             â”‚   â”‚
â”‚  â”‚      p.add(event2(...))                             â”‚   â”‚
â”‚  â”‚      results = await p.gather()                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ParallelExecutionManager                        â”‚
â”‚  â€¢ Manages concurrency (semaphore)                          â”‚
â”‚  â€¢ Creates ParallelEventGroup                               â”‚
â”‚  â€¢ Orchestrates execution                                   â”‚
â”‚  â€¢ Collects results                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ExecutionState  â”‚  â”‚    EventBus      â”‚
â”‚  â€¢ Tracks groups â”‚  â”‚  â€¢ Publishes     â”‚
â”‚  â€¢ Stores meta   â”‚  â”‚    events        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ ParallelEventPrinter â”‚
                     â”‚  â€¢ Detects groups    â”‚
                     â”‚  â€¢ Shows progress    â”‚
                     â”‚  â€¢ Formats output    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. User creates parallel context
   â†“
2. Events added to ParallelEventGroup
   â†“
3. ParallelExecutionManager wraps each with semaphore
   â†“
4. Tasks launched with asyncio.create_task()
   â†“
5. Events emit START events â†’ EventBus
   â†“
6. EventPrinter detects group_id, shows progress
   â†“
7. Events complete, emit COMPLETED â†’ EventBus
   â†“
8. EventPrinter updates progress bar
   â†“
9. Manager collects results
   â†“
10. Group COMPLETED event emitted
    â†“
11. Results returned to user
```

---

## ğŸ“ Implementation Phases

### Phase 1: Core Parallel Execution (Week 1-2)
- [ ] Implement `ParallelEventGroup` model
- [ ] Create `ParallelExecutionManager` 
- [ ] Add parallel metadata to `ExecutionEvent`
- [ ] Implement context manager API
- [ ] Add semaphore-based concurrency control
- [ ] Basic result gathering (list mode)
- [ ] Unit tests

### Phase 2: Event Bus Enhancement (Week 2-3)
- [ ] Optimize event bus for parallel events
- [ ] Add group event emission
- [ ] Ensure thread-safety for concurrent publishing
- [ ] Performance testing

### Phase 3: EventPrinter Enhancement (Week 3-4)
- [ ] Implement `ParallelEventPrinter`
- [ ] Add progress bar rendering
- [ ] Implement real-time mode
- [ ] Implement progress mode
- [ ] Add group statistics

### Phase 4: Advanced Features (Week 4-5)
- [ ] Streaming result mode
- [ ] Dictionary result mode
- [ ] Dependency resolution
- [ ] Error handling strategies
- [ ] Integration tests

### Phase 5: Documentation & Examples (Week 5-6)
- [ ] API documentation
- [ ] Usage examples
- [ ] Migration guide
- [ ] Best practices guide
- [ ] Performance benchmarks

---

## ğŸ§ª Testing Strategy

### Unit Tests
- ParallelExecutionManager execution logic
- Semaphore limiting
- Result gathering in different modes
- Error handling scenarios
- Group lifecycle

### Integration Tests
- Full workflow with EventPrinter
- State recovery with parallel events
- Cache interactions with parallel execution
- Event bus under concurrent load

### Performance Tests
- Scalability with increasing workers
- Memory usage with large parallel groups
- Event emission throughput
- Progress bar rendering performance

### Edge Cases
- Empty parallel groups
- Single event in group
- All events fail
- Mixed success/failure
- Cancellation during execution

---

## ğŸš€ Usage Examples

### Basic Parallel Execution

```python
# Execute multiple independent tool calls
async with execution_state.parallel(max_workers=5) as parallel:
    parallel.add(process_doc1(...))
    parallel.add(process_doc2(...))
    parallel.add(process_doc3(...))
    
results = await parallel.gather()
```

### Named Results

```python
async with execution_state.parallel(max_workers=3) as parallel:
    parallel.add("doc1", process_doc(...))
    parallel.add("doc2", process_doc(...))
    
results = await parallel.gather_dict()
# Access: results["doc1"], results["doc2"]
```

### Streaming Results

```python
async with execution_state.parallel(max_workers=10) as parallel:
    for doc in documents:
        parallel.add(process_doc(doc))
    
    async for result in parallel.stream():
        # Process each result as soon as it's available
        print(f"Processed: {result}")
```

### With Custom Printer

```python
printer = ParallelEventPrinter(
    parallel_mode="progress",
    show_progress=True,
    collapse_parallel=False
)

bus = EventBus()
bus.subscribe(printer)
execution_state = ExecutionState(event_bus=bus)

async with execution_state.parallel(max_workers=5) as p:
    # ... add events
    results = await p.gather()
```

---

## ğŸ”’ Constraints & Considerations

### State Recovery Compatibility
- Parallel groups must be deterministic for replay
- Group IDs should be stable across runs
- Results must be stored in order-independent way
- Replay should maintain parallelism structure

### Cache Interactions
- Parallel cache hits should not block other events
- Cache backend must be thread-safe
- Parallel events should have independent cache keys

### Resource Management
- Respect system limits (file descriptors, memory)
- Provide sensible defaults (max_workers=5)
- Allow global resource limiting
- Monitor and report resource usage

### Event Bus Performance
- Publishing must be efficient under high concurrent load
- Subscribers should not block parallel execution
- Queue-based async handling for heavy subscribers

### Backward Compatibility
- Existing sequential code must continue to work
- No breaking changes to current APIs
- Parallel features opt-in
- Progressive enhancement approach

---

## ğŸ“Š Success Metrics

### Performance
- [ ] 3-5x speedup for 5 independent events (vs sequential)
- [ ] Overhead < 10ms per parallel group
- [ ] Memory usage growth O(n) with event count

### Usability
- [ ] Less than 5 lines of code to enable parallelism
- [ ] Clear error messages for common mistakes
- [ ] Helpful warnings for potential issues

### Reliability
- [ ] 100% test coverage for parallel execution logic
- [ ] Zero race conditions in normal usage
- [ ] Graceful handling of all error scenarios

---

## ğŸ”® Future Enhancements

### Priority-Based Execution
- Different events have different priorities
- High-priority events get preference
- Dynamic priority adjustment

### Dependency Graph Execution
- Automatic dependency resolution from event relationships
- Topological sort for execution order
- Parallel execution of independent subgraphs

### Resource Pooling
- Shared resource pools (DB connections, API clients)
- Automatic resource acquisition and release
- Resource-aware scheduling

### Adaptive Concurrency
- Auto-tune max_workers based on system load
- Monitor event execution times
- Adjust parallelism dynamically

### Distributed Execution
- Execute events across multiple processes/machines
- Distributed event bus
- Centralized result collection

---

## ğŸ“š References

- [asyncio Documentation](https://docs.python.org/3/library/asyncio.html)
- [Semaphore Pattern](https://en.wikipedia.org/wiki/Semaphore_(programming))
- [Task Groups (PEP 654)](https://peps.python.org/pep-0654/)
- [Structured Concurrency](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/)

---

## ğŸ“‹ Decision Summary

Please fill in your decisions above and any additional notes below:

### Additional Considerations:

**Performance Requirements:**
```
[Your notes on expected throughput, latency, resource constraints]
```

**Specific Use Cases:**
```
[Describe your primary use cases for parallel execution]
```

**Integration Points:**
```
No integration yet
```

**Timeline:**
```
Suggest one based on the decisions we've just made. 
```

### Other Suggestions

**Question:** Are there any other parallel execution features you'd like to see?

**Suggestions:**
```
[X] Timeout support for parallel groups
[X] Retry logic for failed parallel events
[X] Circuit breaker pattern for external calls
[X] Parallel execution metrics/observability
[ ] Integration with distributed task queues (Celery, RQ)
[ ] Support for cancellation tokens
[ ] Nested parallel groups
[ ] Other: _________________
```

**Notes:**
```
[Your additional suggestions and notes]
```

---

## âœ… Next Steps

1. **Review this specification** and fill in your decisions
2. **Discuss with stakeholders** any concerns or questions
3. **Prioritize features** - which are must-have vs nice-to-have
4. **Approve to proceed** with implementation
5. **Set up tracking** for implementation progress

---

**Document Status:** â³ Awaiting Decisions  
**Last Updated:** January 23, 2026  
**Owner:** [Your Name]  
**Reviewers:** [List reviewers]
